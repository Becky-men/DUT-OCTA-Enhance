#!/usr/bin/env python3
"""
å®¢æˆ¶è‡ªå®šç¾©åœ–åƒæ¨ç†è…³æœ¬
å°æ ¹ç›®éŒ„ä¸‹testæ–‡ä»¶å¤¾ä¸­çš„åœ–åƒé€²è¡Œå¢å¼·è™•ç†
"""
import os
import sys
import numpy as np
from PIL import Image
import torch
import cv2
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')

# é…ç½®ä¸­æ–‡å­—é«”æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'PingFang HK', 'PingFang SC', 'STHeiti', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'pytorch-CycleGAN-and-pix2pix'))

from models import create_model
from options.test_options import TestOptions


def load_model():
    """åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹"""
    print("æ­£åœ¨åŠ è¼‰æ¨¡å‹...")
    
    # è¨­ç½®é¸é …
    project_root = os.path.dirname(os.path.abspath(__file__))
    checkpoints_dir = os.path.join(project_root, 'pytorch-CycleGAN-and-pix2pix', 'checkpoints')
    
    old_argv = sys.argv
    sys.argv = [
        'test.py',
        '--dataroot', os.path.join(project_root, 'Dataset', 'ROSE'),
        '--checkpoints_dir', checkpoints_dir,
        '--name', 'rose_svc_pix2pix',
        '--model', 'pix2pix',
        '--netG', 'unet_256',
        '--direction', 'AtoB',
        '--dataset_mode', 'rose',
        '--norm', 'batch',
        '--input_nc', '1',
        '--output_nc', '1',
        '--no_dropout',
        '--epoch', '400',  # ä½¿ç”¨æœ€çµ‚æ¨¡å‹
    ]
    
    opt = TestOptions().parse()
    sys.argv = old_argv
    
    # æ‰‹å‹•è¨­ç½®ä¸€äº›é¸é …
    opt.num_threads = 0
    opt.batch_size = 1
    opt.serial_batches = True
    opt.no_flip = True
    opt.display_id = -1
    opt.isTrain = False
    
    # è¨­ç½®è¨­å‚™
    import torch
    opt.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    opt.gpu_ids = []  # ä½¿ç”¨CPU
    
    # å‰µå»ºæ¨¡å‹
    model = create_model(opt)
    model.setup(opt)
    
    if hasattr(model, 'eval'):
        model.eval()
    
    print("âœ… æ¨¡å‹åŠ è¼‰å®Œæˆ")
    return model, opt


def preprocess_image(image_path, target_size=256):
    """é è™•ç†è¼¸å…¥åœ–åƒ"""
    # è®€å–åœ–åƒ
    if image_path.lower().endswith(('.tif', '.tiff')):
        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)
        if img is None:
            # å˜—è©¦ç”¨PILè®€å–
            pil_img = Image.open(image_path)
            img = np.array(pil_img)
    else:
        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    
    if img is None:
        raise ValueError(f"ç„¡æ³•è®€å–åœ–åƒ: {image_path}")
    
    # ç¢ºä¿æ˜¯ç°åº¦åœ–
    if len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # è¨˜éŒ„åŸå§‹å°ºå¯¸
    original_size = img.shape[:2]
    
    # æ­¸ä¸€åŒ–åˆ°0-255
    if img.dtype != np.uint8:
        img = ((img - img.min()) / (img.max() - img.min() + 1e-8) * 255).astype(np.uint8)
    
    # èª¿æ•´å¤§å°
    img_resized = cv2.resize(img, (target_size, target_size), interpolation=cv2.INTER_LINEAR)
    
    # è½‰æ›ç‚ºtensorï¼Œæ­¸ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = torch.from_numpy(img_resized).float()
    img_tensor = (img_tensor / 255.0 - 0.5) / 0.5  # æ­¸ä¸€åŒ–åˆ°[-1, 1]
    img_tensor = img_tensor.unsqueeze(0).unsqueeze(0)  # æ·»åŠ batchå’Œchannelç¶­åº¦
    
    return img_tensor, img, original_size


def postprocess_output(output_tensor, original_size=None):
    """å¾Œè™•ç†è¼¸å‡ºåœ–åƒ"""
    # å¾tensorè½‰æ›å›numpy
    output = output_tensor.squeeze().cpu().detach().numpy()
    
    # å¾[-1, 1]è½‰æ›å›[0, 255]
    output = ((output + 1) / 2 * 255).clip(0, 255).astype(np.uint8)
    
    # å¦‚æœéœ€è¦ï¼Œèª¿æ•´å›åŸå§‹å¤§å°
    if original_size is not None:
        output = cv2.resize(output, (original_size[1], original_size[0]), interpolation=cv2.INTER_LINEAR)
    
    return output


def run_inference(model, input_tensor):
    """é‹è¡Œæ¨¡å‹æ¨ç†"""
    with torch.no_grad():
        # è¨­ç½®è¼¸å…¥
        model.set_input({'A': input_tensor, 'B': input_tensor, 'A_paths': '', 'B_paths': ''})
        # é‹è¡Œæ¨ç†
        model.test()
        # ç²å–è¼¸å‡º
        visuals = model.get_current_visuals()
        output = visuals['fake_B']
    return output


def create_comparison_figure(original, enhanced, save_path, filename):
    """å‰µå»ºå°æ¯”åœ–"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # åŸå§‹åœ–åƒ
    axes[0].imshow(original, cmap='gray', vmin=0, vmax=255)
    axes[0].set_title(f'åŸå§‹åœ–åƒ\n{filename}', fontsize=14, fontweight='bold')
    axes[0].axis('off')
    
    # å¢å¼·åœ–åƒ
    axes[1].imshow(enhanced, cmap='gray', vmin=0, vmax=255)
    axes[1].set_title('GANå¢å¼·çµæœ', fontsize=14, fontweight='bold', color='green')
    axes[1].axis('off')
    
    # å·®ç•°åœ–ï¼ˆé¡¯ç¤ºå¢å¼·æ•ˆæœï¼‰
    # è¨ˆç®—å°æ¯”åº¦æå‡
    orig_contrast = original.std()
    enh_contrast = enhanced.std()
    contrast_change = (enh_contrast - orig_contrast) / (orig_contrast + 1e-8) * 100
    
    # å‰µå»ºå·®ç•°ç†±åœ–
    diff = np.abs(enhanced.astype(float) - cv2.resize(original, (enhanced.shape[1], enhanced.shape[0])).astype(float))
    axes[2].imshow(diff, cmap='hot')
    axes[2].set_title(f'è®ŠåŒ–å€åŸŸç†±åœ–\nå°æ¯”åº¦æå‡: {contrast_change:+.1f}%', fontsize=14, fontweight='bold')
    axes[2].axis('off')
    
    plt.suptitle(f'OCTAåœ–åƒå¢å¼·æ•ˆæœ - {filename}', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    return contrast_change


def main():
    """ä¸»å‡½æ•¸"""
    print("="*70)
    print("å®¢æˆ¶æ¸¬è©¦åœ–åƒæ¨ç†")
    print("="*70)
    
    # è¨­ç½®è·¯å¾‘
    input_dir = os.path.join(os.path.dirname(__file__), 'test')
    output_dir = os.path.join(os.path.dirname(__file__), 'test_results')
    
    # æª¢æŸ¥è¼¸å…¥ç›®éŒ„
    if not os.path.exists(input_dir):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¸¬è©¦ç›®éŒ„ {input_dir}")
        return
    
    # ç²å–æ‰€æœ‰åœ–åƒæ–‡ä»¶
    valid_extensions = ('.tif', '.tiff', '.png', '.jpg', '.jpeg')
    image_files = [f for f in os.listdir(input_dir) 
                   if f.lower().endswith(valid_extensions)]
    
    if not image_files:
        print(f"âŒ éŒ¯èª¤ï¼šæ¸¬è©¦ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°åœ–åƒæ–‡ä»¶")
        return
    
    print(f"\næ‰¾åˆ° {len(image_files)} å€‹æ¸¬è©¦åœ–åƒ:")
    for f in image_files:
        print(f"  - {f}")
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è¼‰æ¨¡å‹
    model, opt = load_model()
    
    # è™•ç†æ¯å€‹åœ–åƒ
    print("\n" + "="*70)
    print("é–‹å§‹è™•ç†åœ–åƒ...")
    print("="*70)
    
    results = []
    
    for filename in image_files:
        print(f"\nè™•ç†: {filename}")
        
        input_path = os.path.join(input_dir, filename)
        
        try:
            # é è™•ç†
            input_tensor, original_img, original_size = preprocess_image(input_path)
            print(f"  åŸå§‹å°ºå¯¸: {original_size}")
            
            # æ¨ç†
            output_tensor = run_inference(model, input_tensor)
            
            # å¾Œè™•ç†ï¼ˆæ¢å¾©åŸå§‹å°ºå¯¸ï¼‰
            enhanced_img = postprocess_output(output_tensor, original_size)
            
            # ä¿å­˜å¢å¼·å¾Œçš„åœ–åƒ
            base_name = os.path.splitext(filename)[0]
            output_path = os.path.join(output_dir, f"{base_name}_enhanced.png")
            Image.fromarray(enhanced_img).save(output_path)
            print(f"  âœ… å¢å¼·åœ–åƒå·²ä¿å­˜: {output_path}")
            
            # å‰µå»ºå°æ¯”åœ–
            comparison_path = os.path.join(output_dir, f"{base_name}_comparison.png")
            contrast_change = create_comparison_figure(original_img, enhanced_img, comparison_path, filename)
            print(f"  âœ… å°æ¯”åœ–å·²ä¿å­˜: {comparison_path}")
            print(f"  ğŸ“Š å°æ¯”åº¦æå‡: {contrast_change:+.1f}%")
            
            results.append({
                'filename': filename,
                'contrast_change': contrast_change,
                'status': 'success'
            })
            
        except Exception as e:
            print(f"  âŒ è™•ç†å¤±æ•—: {str(e)}")
            results.append({
                'filename': filename,
                'status': 'failed',
                'error': str(e)
            })
    
    # å‰µå»ºç¸½è¦½åœ–
    print("\n" + "="*70)
    print("ç”Ÿæˆç¸½è¦½åœ–...")
    print("="*70)
    
    create_overview(output_dir, image_files)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "="*70)
    print("è™•ç†å®Œæˆæ‘˜è¦")
    print("="*70)
    
    successful = [r for r in results if r['status'] == 'success']
    failed = [r for r in results if r['status'] == 'failed']
    
    print(f"\næˆåŠŸ: {len(successful)}/{len(results)}")
    if successful:
        contrasts = [r['contrast_change'] for r in successful]
        print(f"å°æ¯”åº¦æå‡:")
        print(f"  å¹³å‡: {np.mean(contrasts):+.1f}%")
        print(f"  ç¯„åœ: [{min(contrasts):+.1f}%, {max(contrasts):+.1f}%]")
    
    if failed:
        print(f"\nå¤±æ•—: {len(failed)}")
        for r in failed:
            print(f"  - {r['filename']}: {r.get('error', 'Unknown error')}")
    
    print(f"\nâœ… æ‰€æœ‰çµæœå·²ä¿å­˜åˆ°: {output_dir}/")
    print("  - *_enhanced.png: å¢å¼·å¾Œçš„åœ–åƒ")
    print("  - *_comparison.png: å°æ¯”åœ–")
    print("  - overview.png: ç¸½è¦½åœ–")


def create_overview(output_dir, image_files):
    """å‰µå»ºæ‰€æœ‰æ¸¬è©¦åœ–åƒçš„ç¸½è¦½åœ–"""
    n_images = len(image_files)
    if n_images == 0:
        return
    
    # è¨ˆç®—ç¶²æ ¼å¤§å°
    n_cols = min(3, n_images)
    n_rows = (n_images + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols * 2, figsize=(6 * n_cols, 4 * n_rows))
    if n_rows == 1 and n_cols * 2 == 2:
        axes = np.array([[axes[0], axes[1]]])
    elif n_rows == 1:
        axes = axes.reshape(1, -1)
    
    for idx, filename in enumerate(image_files):
        row = idx // n_cols
        col = (idx % n_cols) * 2
        
        base_name = os.path.splitext(filename)[0]
        
        # è®€å–åŸå§‹åœ–åƒ
        input_path = os.path.join(os.path.dirname(__file__), 'test', filename)
        try:
            if filename.lower().endswith(('.tif', '.tiff')):
                orig_img = np.array(Image.open(input_path))
            else:
                orig_img = np.array(Image.open(input_path).convert('L'))
            
            if len(orig_img.shape) == 3:
                orig_img = cv2.cvtColor(orig_img, cv2.COLOR_RGB2GRAY)
        except:
            continue
        
        # è®€å–å¢å¼·åœ–åƒ
        enhanced_path = os.path.join(output_dir, f"{base_name}_enhanced.png")
        try:
            enh_img = np.array(Image.open(enhanced_path).convert('L'))
        except:
            continue
        
        # é¡¯ç¤ºåŸå§‹åœ–åƒ
        axes[row, col].imshow(orig_img, cmap='gray')
        axes[row, col].set_title(f'{base_name}\nåŸå§‹', fontsize=10)
        axes[row, col].axis('off')
        
        # é¡¯ç¤ºå¢å¼·åœ–åƒ
        axes[row, col + 1].imshow(enh_img, cmap='gray')
        axes[row, col + 1].set_title(f'{base_name}\nå¢å¼·', fontsize=10, color='green')
        axes[row, col + 1].axis('off')
    
    # éš±è—ç©ºç™½å­åœ–
    for idx in range(n_images, n_rows * n_cols):
        row = idx // n_cols
        col = (idx % n_cols) * 2
        axes[row, col].axis('off')
        axes[row, col + 1].axis('off')
    
    plt.suptitle('å®¢æˆ¶æ¸¬è©¦åœ–åƒå¢å¼·æ•ˆæœç¸½è¦½', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'overview.png'), dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  âœ… ç¸½è¦½åœ–å·²ä¿å­˜: {os.path.join(output_dir, 'overview.png')}")


if __name__ == '__main__':
    main()

